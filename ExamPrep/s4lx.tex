%\requirepackage{lineno}
\documentclass[11pt]{article}
\usepackage[left=4cm,top=3cm,right=3cm,left=3cm,nofoot]{geometry}  \geometry{a4paper}                   
\usepackage{tipa, apalike, graphicx, amssymb, delarray, epstopdf, amsmath, amsthm, setspace, supertabular, qtree, hyperref, footnote, palatino, url, multicol, hanging, fullpage, supertabular, ulem} 
%, lineno, gb4e} % gb4e messes up math mode. 
\clubpenalty=300 \widowpenalty=300
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%\usepackage[parfill]{parskip} 			%Activate for line, not indent, paragraphs.
\newenvironment{itemise}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\title{Stats 4 Lx Exam Prep}
\author{Richard Littauer \& Lamar Payson}
\date{\today}                          		% Activate to display a given date or no date
\singlespace
\begin{document}
\maketitle

\section*{Class 1}
\begin{itemise}
\item Inferential Stats: modelling, estimate characteristics of whole population, how likely they behave the same way.
\item population: entire collection of events you're interested in. 
\item sample: actual set of observations
\item sampling frame: set of events/participants that can be measured (after pop defined)
\item sampling methods: simple random, systematic, stratified, cluster, convenience, quota.
\begin{itemise}
\item Simple random: each member equal chance of being included in sample. Good if homogenous.
\item Systematic: ordered, at pre-defined steps.
\item Stratified: sub-populations, measurements can very, samples taken {\it from} sub-populations. \\
Used when the population can be isolated and divided into similar groups.
\item Cluster: Split into groups, random sampling from clusters, then they all go back into the whole sample.
\item Convenience: as random as possible. Good for pretests.
\item Quote: predefined categories to be included. Not random. 
\end{itemise}
\item Sampling error = estimation error = sample-to-sample variation due to chance
\item Sampling distribution = the distribution of a stat over repeated sampling from a pop, or the degree of variability between samples expected by chance as a function of the sampling error.
\item Observational linguistics: Variables are controlled, relationships between variables are correlated.
\item Experimental: variables manipulated, how they affect each other is causal. 
\end{itemise}
\section*{Class 2}
\begin{itemise}
\item Variables: things we measure/register or manipulate/control
\item DV: what is measured/registered in the study
\item IV (predictor): what we manipulate. 
\item Levels: one factor in different amount of levels (?)
\item Discrete var: small set of possible values
\item Continuous variables: any value
\item Null Hypothesis (H0) = there is no difference.
\item Alt Hypo. (H1) = there is a relationship.
\item Type 1 error: H0 rejected  = false positive = alpha
\item Type 2 error: H0 accepted even though H1 was true = false negative = beta
\item Scales of measurement: Nominal, Ordinal, INterval, Ratio
\begin{itemise}
\item Nominal: predefined categories
\item Ordinal: ranked for category, but distances not measured
\item Interval: ranked, same difference between
\item Ratio: Interval with an absolute zero
\end{itemise}
\item Discrete (categorical) variables: nominal, ordinal
\item Continuous: interval, ratio scale
\item Likert scale: on a -2...2 scale
\item Can rescale Ratio $\rightarrow$ Interval $\rightarrow$ Ordinal $\rightarrow$ Nominal, but not other way around.
\item Designs: Within, between subjects, mixed desing.
\begin{itemise}
\item Within: Each gets test materal from each condition for all levels of IV. No systematic differences for different sets.
\item Between: Participants in groups: each gets one condition. Need to control for different features across groups.
\item Mixed: Some between, some within. 
\end{itemise}
\item Long data format: Each observation in its own row: Columns code relevant information.
\end{itemise}
\section*{Class 3}
\begin{itemise}
\item Descriptive stats: Statistical procedures to organise, summarise, and simplify data
\item Frequ. Distribution: a distribution of values of the DV that are tabled against their frequency of occurance.
\item Continuous: Normal/gaussian
\item Discrete freq. dist. = binomial, Poisson
\item Freq. Dist types: Normal /$\backslash$ ,bimodal M, negatively skewed /, positively skewed $\backslash$ 
\item Modality: unimodal, bimodal, symmetric (both sides of center)
\item Binomial: dichotomous, mutually exclusive, independence of observations
\item central tendency: median, mode, mean.
\item variation: range, variance, standard deviation
\begin{itemise}
\item mean: sum of values divided by the number of scores (standard equation, good for pop mean, may not be present, affected by extremes)
\item median: the values corresponding to the middle of an ordered vector (unaffected by extreme, does not require assumptions about interval properties, may not be present, no equation)
\item mode: the most commonly occurring value, the highest point in distribution (occurs, useful when needing highest value, may not be near mean, no equation)
\end{itemise}
\end{itemise}
\section*{Class 4}
\begin{itemise}
\item Dispersion/variability: the degree points are distributed around the mean
\item Range: measure of distance from lowest to highest
\item Interquartile ranges: the range of the middle 50\% of the observations
\item Sample variance: sum of the squared deviations about the mean divided by $N-1$
\item Population variance: $\frac{\sum[x-x\prime]^2}{n-1}$
\item Standard deviation(STD, SD): square root of variance. Low = values close to mean. 
\item Barplots = Categorical variables, variability, dispersion, skewness, outliers
\item Histogram = Continuous variables.
\item Null/alt hypothesis: Is there a relationship, how reliable is it, what kind of relationship is it?
\item Error: difference between the observed and true value
\item Systematic error: occurs due to experimental bias or observer
\item random error: due to fluctuations in observations (This is what we can figure out.) 
\item Analytic view: probability is defined in terms of analysis of possible outcomes
\item relative frequency view: probability is defined in terms of past performance
\item Subjective probability: probability is defined in terms of personal subjective belief in the likelihood of an outcome
\item Event: outcome of a trial
\item Independence: (non)occurrence of one event does not affect the probability of another event
\item Mutually exclusive: occurrence of one event precludes the occurrence of another event
\item Exhaustive: a set of events that represents all possible outcomes
\item Additive law of probability of the occurrence of one or more mutually exclusive events
\item Multiplicative lay of the probability of the joint occurrence of independent events
\item Joint probability $p(A,B)$ = p of the co-occurrence of two or more events.
\item Conditional probability $p(A|B)$ = p of one event given the occurrence of some other event
\item Central tendency tests: t tests, chi square, binomial.
\item Regression models: regression, Anova, Kruskall-Wallis. 
\item Correlation: Spearman, Kendall's.
\end{itemise}
\section*{Class 5}
\begin{itemise}
\item Parametric: tests that have assumptions /estimation of pop parameters. Sample/pop variance, probability distribution assumptions. Good if the assumptions are correct.
\item Non-parametric: less reliance on parameter estimation and distribution assumptions. 
\item Significance level = rejection level = $\alpha$ level. The probability with which to reject null. 
\item T distribution: mean is 0, variance greater than 1, close to 1 with many degrees of freedom
\item Central Limit Theorem: When N increases, shape of the sampling dist approaches normal. %I don't really understand slide 8
\item df = degrees of freedom = how much precision an estimate has. 
\item one-tailed: if extreme values only on one side of the dist would be a reason to reject H0.
\item two-tailed: if either side would give a reason. 
\item  one and two sample tests: for continues DV, when distribution is normal, unimodal, symmetric, non-skewed (or the second has a large data size)
\item Wilcoxon signed-rank test: Non-parametric alt to one sample t test. Median diff, use with Skewed distributions. reduced power.
\item $\mu$ is the expected/hypothetical median.
\item For Wilcoxon: if test stat is less or equal to value, null is rejected. 
\item Two sample t test = student's t test: do two samples come from distributions with the same mean?
\item Two vectors for this: data points, predictor (IV)
\item Welch's test: when the samples have unequal variances and/or sample size)
\item Two-sample tt assumptions: sampling methods are the same, samples are independent, each population larger than sample, each sample is from a normal population.
\item Two-sample Wilcoxon: nonparametric alternative for above.
\item paired t-test: two measurements of the same experimental unit. Matched pairs Wilxocon non-para alt.
\end{itemise}
\section*{Class 6}
\begin{itemise}
\item Shapiro-Wilk W Test: H0 - sample from normal. If p value has sig w value, sample not from normal distribution, reject H0. Robust for small, middle-size samples. 
\item regression models: prediction of one DV from one or more IVs.
\item F distribution = continuous probability distribution (used in Anova)
\item intercept: the value of y when x is 0. 
\item Slope: the change un y for a one-unit change in X
\item Regression coefficients: name for both intercept and slope (regression parameters)
\item One way Anova: parametric, Numerical DV and a factor
\item Linear regression: regression in which the relationship between variables is linear
\item Assumptions: normality, homogeneity of variance, independence of observations.
\item Adjusted R Squared = squared correlation coefficient proportion of variance = goodness of fit
\item Kruskal-Wallis test: non-parametric alt for a one way anova. between-group sum of squares from the average ranking. 
\item multiple regression = more than one predictor (IV)
\item main effects +, interactions *
\item Interaction between two factors: all possible combinations of levels of the two factors
\item Between a factor and a continuous variable: describes linear effects of the continuous variable but with different slopes within each group defined by the factor
\item Interaction between two cont var: new regression var that is the product of two
\item Collinearity: when two or more IVs are highly correlated
\item If both/all correlated IVs are used, coefficient estimates fluctuate erratically
\item Overall result of IVs to DVs works, but not individual IVs; may overfit
\end{itemise}
\section*{Class 7}
\begin{itemise}
\item Bonferroni correction: alpha / $n$, where $n$ = number of comparisons: used for mult comparisons for variables in one dataset. 
\item Tukey's HSD (Honest(ly) significant difference) test: assumption; all levels have an equal number of observations (aov, not lm).
\item Chi$^2$ dist for categorical data
\item Cumulative probability: asymmetric distribution with minimum of 0 and no max value
\item the mean of a chi$^2$ distribution is equal to the number of degrees of freedom; variance = to times the number of $df$; as $df$ increases, the chi$^2$ curve approahces a normal dist.
\item Chi$^2$ is non-parametric. If the value is equal or greater than the p value, the null is rejected.
\item If less than 50, use Fisher's test, which is a hypergeometic dist to calc the prob of the obs outcome. 
\item Logistic regression = class of generalized linear models (including liner models and logistic models); extension to least square regression models, but parameter estimation differs.
\item least square = minimises the sum of the squared error
\item log reg = MLE
\item likelihood fx = the prob of the obs data, expressed as a fx of the para. 
\item lrm() = log reg with binary DV
\item glm() = same, but requires number of success/failures
\item Correlation = relationship between two random variables in range 1/-1 (0 = no cor.)
\item Correlation coefficient = a measure of the relationship
\item Parametric = Pearson. Non-para = Spearman, kendall
\end{itemise}

















%\bibliographystyle{apasoft}
%\bibliography{/Users/richardlittauer/Desktop..}
\end{document}