%\requirepackage{lineno}
\documentclass[11pt]{article}
\usepackage[left=4cm,top=3cm,right=3cm,left=3cm,nofoot]{geometry}  \geometry{a4paper}                   
\usepackage{tipa, apalike, graphicx, amssymb, delarray, epstopdf, amsmath, amsthm, setspace, supertabular, qtree, hyperref, footnote, palatino, url, multicol, hanging, fullpage, supertabular, ulem} %, lineno, gb4e} 
%gb4e messes up math mode. 
\clubpenalty=300 \widowpenalty=300
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%\usepackage[parfill]{parskip} 			%Activate for line, not indent, paragraphs.

\newenvironment{itemise}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}


\title{FSLT Examination Preparation}
\author{Richard Littauer}		% Add your name here
\date{\today}                          		% Activate to display a given date or no date
\singlespace
\begin{document}
\maketitle
\tableofcontents

\section*{Overview}
%The point is to have a latex analysis of the slides or relevant facts to know for each section, and subsequently, each slide. We can work on this collaboratively - remember to push and pull the sections you're working on, and try to make sure that you aren't clashing with other's doing the same work. 
Format of the exam:
\begin{itemise}
\item 9 short questions for each section. These are obligatory. 
\item 5 long questions from 6 sections. You can pick up 3 from them. 
\begin{itemise}
\item linguistic foundations
\item finite-state automata
\item parsing
\item seminar
\item statistical NLP
\item speech
\end{itemise}
\end{itemise}
\newpage
\section{Map of the Field (Uzkoreit) 24.10-26.10}

Was anything actually covered in these two sessions? Does anyone remember? 

\section{Linguistics Foundation (Delogu) 28.10-09.11}
\subsection{Linguistic Foundations I}
Properties of human language: Arbitrary (no relation), Discrete (units), Duality (sounds, meanings), Displacement (things not here), Creative (infinitely so).

- The difference between linguistic competence and linguistic performance
\begin{itemize}
 \item \textbf{Linguistic competence} -The implicit knowledge a language user has of his language, which enables him to produce and understand any possible sentence of that language
 \item \textbf{Linguistic performance} - The actual use of language in real situations, which is conditioned by physiological and psychological constraints (memory limitations, shifts of attention, etc.)
\end{itemize}
- The difference between sentence gramaticality and acceptability
\begin{itemize}
 \item \textbf{Gramaticality} - A sentence is gramatical if it is formed according to the gramar of the language
\item \textbf{Acceptability} - A sentence is acceptable if it 'sounds good' to a native speaker
\end{itemize}
note: a sentence can be gramatical but unnacceptale (e.g. hard to process)
\\\\
- Four definititions of \textbf{grammar}
\begin{itemize}
 \item The linguistic rules of a language that every native speaker intuitevly knows - Linguistic competence
\item A model of the linguistic rules that every speaker of a language intuitively knows - A theory of linguistic competence 
\item The rules and principles that describe the linguistic behavior of native speakers - Descriptive grammar
\item - The rules and principles that prescribe the linguistic behavior of native speakers, according to some authority - Prescreptive grammar
\end{itemize}
- Linguistic structuralism and the \textbf{inductive method} 
\begin{itemize}
 \item Structural linguistics thus involves collecting a corpus of utterances and then attempting to classify all of the elements of the corpus at their different linguistic levels
\item linguistic constituents were identified by the set of all contexts in which they can occur
\end{itemize}
- Chomsky and the \textbf{Generative Program}
\begin{itemize}
\item A rejection of structuralism and a redefinition of the object of investigation
\item Developed from two key observations:
  \begin{enumerate}
  \item Linguistic competence: people are able to understand and produce an infinite number of grammatical sentences
    \begin{itemize}
    \item The inductive method used by the Structuralists was inadequate (and unable to account for key linguistic phenomena like structural ambiguity)
    \end{itemize}
  \item Language acquisition: children are able to ‘learn’ their language perfectly, even though they are exposed to defective inputs
    \begin{itemize}
    \item The idea that language is learnt through stimulus-response processes, as argued by the Behaviorists (e.g., Skinner 1957) was no longer tenable
    \end{itemize}
  \end{enumerate}
\item The task of a linguist is comparable to a child’s acquisition of linguistic competence.
  \begin{itemize}
  \item Since children do not acquire language inductively, the new methodology must be deductive
  \end{itemize}
\item The object of investigation is no longer the product language as represented by corpora but the linguistic competence of native speakers
\item Linguistic competence is an internalized (mentally- represented) grammar capable of generating an infinite number of grammatical sentences and none of the ungrammatical ones
\item The goal of linguistics is to provide an adequate grammar as a model of linguistic competence
\item Levels of adequacy: Observational, descriptive, explanatory. 
\item Generative Grammar: grammars must be precise, tested against invented data, must be a theory of human competence. 
\item Controversial chomskyan claims: all languages are similar, as linguistic knowledge is innate, and the central problem is acquistion. 
\item GG: Standard, Principles and Parameters, Government and Binding, X-Bar, Miminalism.
\item Non-GG: LFG, HPSG, TAG, CG, Functional, Cognitive...
\end{itemize}

\subsection{Linguistic Foundations II - Learn you a morphology}
\begin{itemize}
 \item \textbf{Morphology} - The study of word structure and word formation
\end{itemize}

- What is a word?
\begin{itemize}
 \item \textbf{Lexeme} - An abstract decontextualized vocabulary item with a core meaning (e.g. {\it see})
 \item \textbf{Word form} - A particular physical realization of a lexeme in text or writting (e.g. {\it sees, saw, seen}) 
 \item \textbf{Grammatical word} - A representation of a lexeme that is associated with certain morpho-syntactic properties
\end{itemize}
- What is a morpheme
\begin{itemize}
 \item {\bf Morpheme} - The minimal, indivisible units of semantic content or grammatical function which words are made up of
  \begin{itemize}
   \item e.g. printable $\rightarrow$ print + able
  \end{itemize}
  \item Morphemes are made manifested in a sequence of sounds called {\bf morphs}
  \begin{itemize}
    %I'm too lazy to figure out how to do ipa symbols
    \item Different pairings between a morpheme and morphs is possible (i.e. homophones e.g. /sait/ $\rightarrow$ site, cite, sight and allomorphs e.g. 'past-tense marker' $\rightarrow$ /id/, /d/, /t/)  
  \end{itemize}
\end{itemize}
- Types of morphemes
\begin{itemize}
 \item Root - Carry a lexical meaning and usually belong to a lexical category
 \item Inflectional - Carry gramatical meaning (e.g. case, number, person, gender, etc.)
 \item Derivational - Serve to form complex words and can change the lexical category of the word it is attached to (e.g. -able $/rightarrow$ do + able $/rightarrow$ doable, verb to adjective)
 \item Free - Can stand alone, don't need to be attached
  \begin{itemize}
    \item Open class - Content words (nouns, adjectives, verbs, adverbs)
    \item Closed class - Function words  (conjunctions, prepositions, articles, pronouns, auxiliaries)
  \end{itemize}
 \item Bound - have to be attached to another morpheme (e.g. suffixes)
\end{itemize}
Morphemes have allomorphs, which are either phonologically (s/z), grammatically (weep/wept), or lexically conditioned (ox/oxen).
- {\bf Portmanteau morphemes}
\begin{itemize}
 \item a morph that represents several morphemes (e.g. walk-{\bf s} where -s is a morpheme for third person, present tense, and singular)
\end{itemize}
- {\bf Inflectional} vs. {\bf Derivational} morphology
\begin{itemize}
 \item Inflectional - determines the appropriate form of a morpheme in a given context
  \begin{itemize}
   \item Marks grammatical distinctions (e.g. singular/plural) 
   \item Is required by syntactic criteria (e.g. English nouns are required to have number)
   \item Some inflectional processes:
    \begin{itemize}
     \item Affixation - Addition of an inflectional affix (e.g. -s plural marker)
     \item Internal change - Substitution of one nonmorphemic segment with another to mark a grammatical contrast
     \item Suppletion - Replacement of a morpheme with an entirely different morpheme to mark grammatical contrast (e.g. forms of be - am, is, are, were)
    \end{itemize}
  \end{itemize}
 \item Derivational (word formation) - Creates complex words by joining free and bound morphemes 
  \begin{itemize}
   \item Are opitional, i.e. not required by syntactic criteria
   \item Some word formation processes:
    \begin{itemize}
     \item Compounding - combining free morphemes to make complex words (e.g. dishwasher)
     \item Derivation - combining a base morpheme with a derivational affix (e.g. un-tie, fear-less)
    \end{itemize}
  \end{itemize}
\end{itemize}
- How to distinguish between inflection and derivation
\begin{itemize}
 \item Category change - inflection doesnt change POS, meaning of the word it is applied to, derivation does (thereby creating new words)
 \item Ordering - derivational affixes must combine with the base before inflectional affixes (e.g. dish + washer + s)
 \item Productivity - inflectional affixes are more productive (i.e. easily apply new appropriate bases)
\end{itemize}
- How are morphemes recognized ({\bf Morpheme-based} vs {\bf Lexeme-based} models)
\begin{itemize}
 \item Morpheme-based models
  \begin{itemize}
   \item basic morphological building block is the morpheme
   \item both free and bound morphemes stores in the lexicon with their meaning and grammatical category
   \item complex words generated by concatenation
  \end{itemize}
 \item Lexeme-based models
  \begin{itemize}
   \item basic morphological building block is the lexeme
   \item bound morphemes are not stored in the lexicon, but rather as part of lexeme based morphological rules (e.g. $[X]_v \rightarrow [[X]_v er]_n$ one who does X ) 
   \item motivated by the existence of non-concatenative morphology
  \end{itemize}
\end{itemize}
In the lexicon: For each item: Semantic, categorial, subcategorial, phonological information.\\
Identifying Parts of speech: Notational (semantic), Distributional (syntactic), Formal (morphology.)\\
Closed vs. Open classes.
EXERCISE QUESTION:

\begin{enumerate}
 \item For the following list of words, 1) give the number of morphemes, 2) number of syllables, 3) identify the root, 4) identify the derivational morpheme(s), 5) identify the inflectional morphemes (suffixes): only, unpacked, graphically, bookshops, healthier, disappearing, coldest, proven, John's, mispronounces, actors, fineger
 \item Give an example of a portmanteau morpheme in your native language (ANSWER: -s as in walk-s - morpheme for third-person, singular, and present tense)
\end{enumerate}

\subsection{Linguistics Foundations III - Syntax}
\begin{itemize}
 \item {\bf Transformational}
  \begin{itemize}
   \item The ultimate goal of ling. theory is to model language acquisition and linguistic competence
   \item Developed from Chomskys' assumption that a theory of grammar is a moel of the mental representation of linguisitic knowledge
   \item Developes in several stages:
    \begin{itemize}
     \item {\bf Standard theory} (and its extensions and revisions)
      \begin{itemize}
       \item Three basic ingredients:
	\begin{enumerate}
	 \item {\bf Phrase structure rules} - used to break down a sentence into its constituent parts (i.e. syntactic categories)
	 \item {\bf Transformational rules} - moving, inserting or deleting constituents and compining phrase markers in oder to preserve meaning
	 \item {\bf The Lexicon} - contains information about lexical items (e.g. phonological, categorial, semantic info...)
	\end{enumerate}
       \item PS Rules + Lexicon $\rightarrow$ Deep Structure $\rightarrow$ Transformational rules $\rightarrow$ Surface Structure
       \item {\bf Deep structure} - represents the core semantic representation of a sentence
       \item {\bf Surface structure}
	\begin{enumerate}
	 \item mapped from the deep structure via transformational rules, the surface structure tree's terminal yield is then a grammatical sentence in the language in question. 
	 \item Unlike deep structure, the surface structure trees generated by the transformational rules may represent passive, interrogative, and inflected sentences.
	 \item Different surface structures convey the same meaning when they share the same underlying deep structure
	 \item A surface sentence conveys more than one meaning wen it is associated with more than one deep structure
	 \item The base component (i.e. PS rules and the syntactic categories on which they operate to generate deep structures of sentences) is universal
	 \item The diversity of languages is due to the idiosyncracies of the lexicon and transformational rules for a given language
        \end{enumerate}
       \item Two weak points:
	\begin{enumerate}
	  \item the 'universal` syntactic categories were too specific to be universal (e.g. not all languages have prepositions) and not specific enough to categorize all lexical items of a language (e.g. 'so' and 'there' dont belong to any categories hypothesized as universal)
	  \item Transformational rule turned out too powerful allowing change meanings as in e.g. `Everyone in this room speaks two languages.' $\rightarrow$ 'Two languages are spoken by everyone in this room.'
	  \end{enumerate}
       \item ...Led to a desire to restrict transformational rules via: GB/PPT and Non-Transformational models (e.g. LFG, HPSG,...)
      \end{itemize}     
     \item {\bf Government and Binding / Principles and Parameters Theory} $\rightarrow$ {\bf Minmilast program}
     \begin{itemize}
      \item {\bf Principles} - Universal grammar rules (e.g. A sentence must have a subject)
      \item {\bf Parameters} - Markers or switches that are turned on/off for individual languages (e.g. head-final parameter off for engllish)
     \end{itemize}
     \item key idea: It is no longer syntactic categories that are claimed to be universal, but rather features
    \end{itemize}
  \end{itemize}
 \item {\bf Non-transformational} (lexicalist)
  \begin{itemize}
   \item A ling. theory must be descriptively adequate, computationally implementable and/or psychollogically plausible
   \item example: LFG and HPSG are highly lexical non-transformational grammars in which grammaticality is determined by satisfaction of simultaneous constraints (i.e. constraint-based)
    \begin{itemize}
     \item {\bf LFG}
      \begin{itemize}
       \item {\bf Lexical component} able to account for transformational phenomena
       \item {\bf c-structure}: phrase-structure tree serving as the basis for phonological interpretation
       \item {\bf f-structure}: hierarchical attribute-value matrix representing underlying grammatical relations (SUBJ, OBJ) and serving as the basis for semantic interpretation
      \end{itemize}
     \item {\bf HPSG}
      \begin{itemize}
       \item Seek to integrate syntax and semantics
       \item A system of signs (words, phrases, sentences) which are related to the signified (meaning)
       \item Signs are represented in feature structures consisting of attributes and values
      \end{itemize}
    \end{itemize}
  \end{itemize}
 \item {\bf Cognitive} 
  \begin{itemize}
   \item Language is the product of more general cognitive abilities and a ling. theory should describe how these cognitive abilities ae used in ling. behavior
   \item Semantics, rather than syntax is the basis of linguistic analysis
   \item Example: {\bf Cognitive Grammar}
    \begin{itemize}
      \item Basic unit of grammar is {\bf symbolic assembly} which associates semantic and phonological representations
      \item No reference to syntactic categories
    \end{itemize}
  \end{itemize}
\end{itemize}
- {\bf Descriptive} vs. {\bf Explanatory} adequacy
\begin{itemize}
 \item {\bf Descriptive} - Theory's rules account for all observed data, rules produce all and only the well-formed constructs of the grammar
 \item {\bf Explanatory} - Theory provides a principled choice between competing descriptions and deals with the uttermost underlying structure
\end{itemize}

\subsection{Linguistic Foundations IV - Semantics}
- Whats all this, then?
\begin{itemize}
 \item Meaning can be seen as the relation between linguistic signs (words, phrases, sentences) and the things they represent (concepts, things in the world, situations, etc.)
 \item Linguistic meaning can be investigated via: 
  \begin{itemize}
   \item Semantic relations between words
    \begin{itemize}
     \item E.g. {\bf Lexical semantics} - the study of lexical meaning
     \item The relationship between words and meanings is arbitrary
      \begin{itemize}
       \item Same word, diff meaning - Homonomy vs. polysemy
       \item Diff. words, same meaning - Synonymy
       \item {\bf Homonymy} - Two words that sound the same but have diff. meanings (e.g. 'bank' - financial institution OR side of a river
       \item {\bf Polysemy} - One word, multiple related meanings (e.g. 'bank' - financial instution OR building housing a financial institution) 
       \item {\bf Qualia roles} - Account for the multiple ways we can represent an object (e.g. Constitutive, Agentitive, Telic)
       \item Types of {\bf antonyms}:
	\begin{enumerate}
	 \item {\bf Complementary pairs} - Presence of one signifies the absence of the other (e.g. married/single)
	 \item {\bf Gradable pairs} - Gradual transition between poles (e.g. hot/cold)
	 \item {\bf Relational pairs} - Same situation from opposite points of view (e.g. buyer/seller)
	\end{enumerate}
       \item {\bf Hyponym} - Words naming subclasses (is-a) (e.g. 'Terrier' is a hyponym for 'dog')
       \item {\bf Meronym} - Words meaning parts (has-a) (e.g. Finger is meronym of hand) 
      \end{itemize}
    \end{itemize}
   \item Semantic relations between sentences
    \begin{itemize}
     \item E.g. {\bf Truth-conditional semantics} - We know the meaning of a sentence when we know what the world must be like for that sentence to be true (i.e. {\bf truth-conditions})
     \item Relations between sentences:
      \begin{itemize}
       \item {\bf Entailment}
	\begin{enumerate}
	 \item A entails B iff whenever A is true, B is true (e.g. `I have 2 children' $\models$ 'I have 1 child')
	 \item Can be extended to the hyponym/hypernym relation (e.g. 'I have a terrier' $\models$ 'I have a dog')
	\end{enumerate}
       \item {\bf Equivalence}
	\begin{enumerate}
	 \item A and B are equivalent iff A entails B and B entails A
	 \item works with synonymous words (e.g. 'A died' is equivalent to 'A passed away')
	\end{enumerate}
       \item {\bf Contradiction}
	\begin{enumerate}
	 \item A and B are in contradiction if they cannot be true at the same time
         \item works with antonyms (e.g. 'John is married' is in contradiction to 'John is single')
	\end{enumerate}
       \item {\bf Presupposition}
	\begin{enumerate}
	 \item B pressuposes A if for B to be true, A must be true
	 \item e.g. 'John has quit smoking' presupposes `John was a smoker'
	 \item Presupposition survives the {\bf negation test} (e.g.John has not quit smoking still presupposes 'John was a smoker')
	 \item NOTE: this is not the case for entailment
	 \item Presupposition triggers:
	  \begin{enumerate}
	   \item Lexical triggers: (regret, realize, know, stop, start, blame, fault)
	   \item Cleft constructions: (It was John that ate the sandwhich -> Someone ate the sandwhich)
	  \end{enumerate}
	 \item Words can have presuppositions (A sleeps -> A is capble of sleeping [c.f. green ideas sleep furiously])
	\end{enumerate}
       \item {\bf Implicature}
	\begin{enumerate}
	 \item A implicates B if A suggests B is true without entailing it
	 \item e.g. 'Some students passed' implies 'Not all students passed'
         \item ...but implicatures are cancleable
	 \item e.g. 'Some students passed, in fact all students passed.'
	\end{enumerate}
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}
- Ambiguity
\begin{itemize}
 \item 3 types:
  \begin{enumerate}
   \item Lexical
    \begin{itemize}
     \item Word sense ambiguity (e.g. John {\bf made} her duck [made can mean cook, create, etc.])
     \item POS ambiguity (e.g. John made {\bf her duck} [her may be personal or possessive pronoun])
    \end{itemize}
   \item Structural - When a part of a sentnece may be interpreted as having different structures (e.g. She saw the man with the binoculars)
   \item Scope - e.g. Everyone loves someone
  \end{enumerate}

\end{itemize}



\section{ Cognitive Foundations (Crocker) 14.11-18.11}
\subsection{Cognitive Foundations I}
\begin{itemise}
 \item Cognitive Linguistics: First language aquisition, use of language, evolution of language.
 \item Nature vs. Nurture:
 \begin{itemise}
	\item Nature (Chomsky): Language-specific genetic makeup, explains common features, aquisition despite poverty of stimulus.
	\item Nurture (Emergentists: Elman, Bates): solely from experience, language adopted to be adaptable, simpler.
 \end{itemise}
 \item UG: parameters that are set, coincides with language localisation in Brain.
 \item Poverty of stimulus: language unlearnable from evidence alone
 \item Children ignore negative evidence
 \item Creoles and NSL taken as signs of growth despite poverty of stimulus.
 \item Language Acquisition Device: takes data, makes competent grammar.
 \item Challenging Nativism: Most people believe in some innateness, poverty of stimulus overstated, sophisticated statistical techniques possible.
 \item Language relativity (Sapri, Whorf, Lakoff, Levinson): Language influences thought (culture)
 \item Linguistic autonomy (Chomsky, Pinker, Fodor): We all have knowledge of language, different from thought.
 \item Geocentric languages overview: Pompurra.
 \item Modularity in the brain: Does language exist separately or not? We're not sure.
 \item Distinct: Representational (storage) vs. procedural (access)  autonomy. 
\end{itemise}


%\section{Technological Foundations (Busemann) 21.11-25.11}
%
\section{Finite State Methods for Lexicon \& Morphology (Kiefer) 28.11-02.12}
NOTE: I didn't have time to draw a bunch of FSA transition grpahs by hand, so if you're unfimilar with how creating or interpreting transition graphs, you'll have to take a look at the slides.

\subsection{ 1st Lecture }

- Finite Automata: Formal Definition
\begin{itemize}
 \item An FA is a tuple $A=<Q, \Sigma , \delta , q_0 , F>$ where:
  \begin{itemize}
   \item ${\bf Q}$ is a finite non empty {\bf set of states}
   \item ${\bf \Sigma}$ is a finite {\bf alphabet of input letters}
   \item ${\bf \delta}$ is a {\bf transition function} $Q \times \Sigma \rightarrow Q $
   \item ${\bf q_0 \in Q}$ is the {\bf initial state}
   \item ${\bf F \subseteq Q}$ is a set of {\bf final states}
  \end{itemize}
\end{itemize}
- String Acceptance
\begin{itemize}
 \item An FA {\bf accepts} an input string $s$ if there is a sequence of states $p_1 , p_2 , \dotsi , p_{|s|} \in Q$ such that
  \begin{enumerate}
   \item $p_1 = q_0$, the start state
   \item $\delta(p_i , s_i ) = p_{i+1}$, where $s_i$ is the $i$-th character is $s$
   \item $p_{|s|} \in F$, i.e. is a final state
  \end{enumerate}
 \item The set of strings accebted by an automaton is the accepted {\bf language}
\end{itemize}
- Nondetermenistic Automata
\begin{itemize}
 \item $\delta$ is a {\bf transition relation} (i.e. not necessarily a {\bf total function})
 \item $\delta : Q \times \Sigma \cup \{ \epsilon \} \rightarrow \mathcal{P}(Q)$, where $\mathcal{P}(Q)$ is the powerset of $Q$
 \item allows transitions from one state into several possible states with the same inut symbol
 \item can have transitions labeled $\epsilon$ which represent the empty string
 \item for every NFA, an equivalent DFA can be constructed
\end{itemize}
- Converting NFA to DFA via the subset construction algorithm
\begin{itemize}
 \item Main Idea: Simulate `in parallel' all possible moves the automaton can make
 \item Uses 2 operations on states/state-sets of the NFA
 \begin{enumerate}
  \item $\epsilon$-$closure(T)$ : Set of states reachable from any $s$ in $T$ on $\epsilon$-transitions
  \item $move(T, a)$ : Set of states to which there is a transition from state in $T$ on an input symbol $a$
 \end{enumerate}
 \item NOTE: I don't like reading algorithms, and there is a nice example in the slides, that makes it clear without having to read through the pseudocode. I would recommend reading the example. finiteautomat1.pdf slides 16 - 24
\end{itemize}

\subsection{2nd Lecture}




\section{Parsing (Zhang) 05.12-09.12}

- Bottom-up vs. Top-Down parsing
\begin{itemize}
 \item Bottom-up - Start from input sequence and apply grammar rules to build tree upwards
 \item Top-down - Start from the start symbol and expand the tree with grammar rules
\end{itemize}
- CKY Algorithm
\begin{itemize}
 \item A bottom-up chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$ 
 \item Grammar must be in CNF
 \item Online javascript example: http://www.diotavelli.net/people/void/demos/cky.html
\end{itemize}
- Earley Algorithm
\begin{itemize}
 \item Top-Down chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$
 \item Example (pdf, slides 18-36): \\http://www.coli.uni-saarland.de/~yzhang/rapt-ws1112/slides/schmidt.pdf
\end{itemize}
- Inside Outside Algorithm
\begin{itemize}
 \item 
\end{itemize}


%\section{Statistical NLP (Language Models) (Klakow, Wiegand) 12.12-23.12}
%
%\section{Prosodic Models for Speech Technology (Moebius) 09.01}
%
%\section{Speech Synthesis (Moebius, Lasarcyk) 11.01-16.01}
%
%\section{Automatic Speech Recognition (Moebius) 18.01}
%
%\section{Corpora for speech technology (Lasarcyk) 20.01}
%
%\section{Semantics (Pinkal) 23.01-3.02}
%
%\section{Discourse \& Dialogue (Sporleder) 06.02-08.02}


\end{document}