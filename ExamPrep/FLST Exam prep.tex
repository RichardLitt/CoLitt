%\requirepackage{lineno}
\documentclass[11pt]{article}
\usepackage[left=4cm,top=3cm,right=3cm,left=3cm,nofoot]{geometry}  \geometry{a4paper}                   
\usepackage{tipa, apalike, graphicx, amssymb, delarray, epstopdf, amsmath, amsthm, setspace, supertabular, qtree, hyperref, footnote, palatino, url, multicol, hanging, fullpage, supertabular, ulem} %, lineno, gb4e} 
%gb4e messes up math mode. 
\clubpenalty=300 \widowpenalty=300
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%\usepackage[parfill]{parskip} 			%Activate for line, not indent, paragraphs.

\newenvironment{itemise}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}


\title{FSLT Examination Preparation}
\author{Richard Littauer}		% Add your name here
\date{\today}                          		% Activate to display a given date or no date
\singlespace
\begin{document}
\maketitle
\tableofcontents

\section*{Overview}
%The point is to have a latex analysis of the slides or relevant facts to know for each section, and subsequently, each slide. We can work on this collaboratively - remember to push and pull the sections you're working on, and try to make sure that you aren't clashing with other's doing the same work. 
Format of the exam:
\begin{itemise}
\item 9 short questions for each section. These are obligatory. 
\item 5 long questions from 6 sections. You can pick up 3 from them. 
\begin{itemise}
\item linguistic foundations
\item finite-state automata
\item parsing
\item seminar
\item statistical NLP
\item speech
\end{itemise}
\end{itemise}
\newpage
\section{Map of the Field (Uzkoreit) 24.10-26.10}

Was anything actually covered in these two sessions? Does anyone remember? 

\section{Linguistics Foundation (Delogu) 28.10-09.11}
\subsection{Linguistic Foundations I}
Properties of human language: Arbitrary (no relation), Discrete (units), Duality (sounds, meanings), Displacement (things not here), Creative (infinitely so).

- The difference between linguistic competence and linguistic performance
\begin{itemize}
 \item \textbf{Linguistic competence} -The implicit knowledge a language user has of his language, which enables him to produce and understand any possible sentence of that language
 \item \textbf{Linguistic performance} - The actual use of language in real situations, which is conditioned by physiological and psychological constraints (memory limitations, shifts of attention, etc.)
\end{itemize}
- The difference between sentence gramaticality and acceptability
\begin{itemize}
 \item \textbf{Gramaticality} - A sentence is gramatical if it is formed according to the gramar of the language
\item \textbf{Acceptability} - A sentence is acceptable if it 'sounds good' to a native speaker
\end{itemize}
note: a sentence can be gramatical but unnacceptale (e.g. hard to process)
\\\\
- Four definititions of \textbf{grammar}
\begin{itemize}
 \item The linguistic rules of a language that every native speaker intuitevly knows - Linguistic competence
\item A model of the linguistic rules that every speaker of a language intuitively knows - A theory of linguistic competence 
\item The rules and principles that describe the linguistic behavior of native speakers - Descriptive grammar
\item - The rules and principles that prescribe the linguistic behavior of native speakers, according to some authority - Prescreptive grammar
\end{itemize}
- Linguistic structuralism and the \textbf{inductive method} 
\begin{itemize}
 \item Structural linguistics thus involves collecting a corpus of utterances and then attempting to classify all of the elements of the corpus at their different linguistic levels
\item linguistic constituents were identified by the set of all contexts in which they can occur
\end{itemize}
- Chomsky and the \textbf{Generative Program}
\begin{itemize}
\item A rejection of structuralism and a redefinition of the object of investigation
\item Developed from two key observations:
  \begin{enumerate}
  \item Linguistic competence: people are able to understand and produce an infinite number of grammatical sentences
    \begin{itemize}
    \item The inductive method used by the Structuralists was inadequate (and unable to account for key linguistic phenomena like structural ambiguity)
    \end{itemize}
  \item Language acquisition: children are able to ‘learn’ their language perfectly, even though they are exposed to defective inputs
    \begin{itemize}
    \item The idea that language is learnt through stimulus-response processes, as argued by the Behaviorists (e.g., Skinner 1957) was no longer tenable
    \end{itemize}
  \end{enumerate}
\item The task of a linguist is comparable to a child’s acquisition of linguistic competence.
  \begin{itemize}
  \item Since children do not acquire language inductively, the new methodology must be deductive
  \end{itemize}
\item The object of investigation is no longer the product language as represented by corpora but the linguistic competence of native speakers
\item Linguistic competence is an internalized (mentally- represented) grammar capable of generating an infinite number of grammatical sentences and none of the ungrammatical ones
\item The goal of linguistics is to provide an adequate grammar as a model of linguistic competence
\item Levels of adequacy: Observational, descriptive, explanatory. 
\item Generative Grammar: grammars must be precise, tested against invented data, must be a theory of human competence. 
\item Controversial chomskyan claims: all languages are similar, as linguistic knowledge is innate, and the central problem is acquistion. 
\item GG: Standard, Principles and Parameters, Government and Binding, X-Bar, Miminalism.
\item Non-GG: LFG, HPSG, TAG, CG, Functional, Cognitive...
\end{itemize}

\subsection{Linguistic Foundations II - Learn you a morphology}
\begin{itemize}
 \item \textbf{Morphology} - The study of word structure and word formation
\end{itemize}

- What is a word?
\begin{itemize}
 \item \textbf{Lexeme} - An abstract decontextualized vocabulary item with a core meaning (e.g. {\it see})
 \item \textbf{Word form} - A particular physical realization of a lexeme in text or writting (e.g. {\it sees, saw, seen}) 
 \item \textbf{Grammatical word} - A representation of a lexeme that is associated with certain morpho-syntactic properties
\end{itemize}
- What is a morpheme
\begin{itemize}
 \item {\bf Morpheme} - The minimal, indivisible units of semantic content or grammatical function which words are made up of
  \begin{itemize}
   \item e.g. printable $\rightarrow$ print + able
  \end{itemize}
  \item Morphemes are made manifested in a sequence of sounds called {\bf morphs}
  \begin{itemize}
    %I'm too lazy to figure out how to do ipa symbols
    \item Different pairings between a morpheme and morphs is possible (i.e. homophones e.g. /sait/ $\rightarrow$ site, cite, sight and allomorphs e.g. 'past-tense marker' $\rightarrow$ /id/, /d/, /t/)  
  \end{itemize}
\end{itemize}
- Types of morphemes
\begin{itemize}
 \item Root - Carry a lexical meaning and usually belong to a lexical category
 \item Inflectional - Carry gramatical meaning (e.g. case, number, person, gender, etc.)
 \item Derivational - Serve to form complex words and can change the lexical category of the word it is attached to (e.g. -able $/rightarrow$ do + able $/rightarrow$ doable, verb to adjective)
 \item Free - Can stand alone, don't need to be attached
  \begin{itemize}
    \item Open class - Content words (nouns, adjectives, verbs, adverbs)
    \item Closed class - Function words  (conjunctions, prepositions, articles, pronouns, auxiliaries)
  \end{itemize}
 \item Bound - have to be attached to another morpheme (e.g. suffixes)
\end{itemize}
Morphemes have allomorphs, which are either phonologically (s/z), grammatically (weep/wept), or lexically conditioned (ox/oxen).
- {\bf Portmanteau morphemes}
\begin{itemize}
 \item a morph that represents several morphemes (e.g. walk-{\bf s} where -s is a morpheme for third person, present tense, and singular)
\end{itemize}
- {\bf Inflectional} vs. {\bf Derivational} morphology
\begin{itemize}
 \item Inflectional - determines the appropriate form of a morpheme in a given context
  \begin{itemize}
   \item Marks grammatical distinctions (e.g. singular/plural) 
   \item Is required by syntactic criteria (e.g. English nouns are required to have number)
   \item Some inflectional processes:
    \begin{itemize}
     \item Affixation - Addition of an inflectional affix (e.g. -s plural marker)
     \item Internal change - Substitution of one nonmorphemic segment with another to mark a grammatical contrast
     \item Suppletion - Replacement of a morpheme with an entirely different morpheme to mark grammatical contrast (e.g. forms of be - am, is, are, were)
    \end{itemize}
  \end{itemize}
 \item Derivational (word formation) - Creates complex words by joining free and bound morphemes 
  \begin{itemize}
   \item Are opitional, i.e. not required by syntactic criteria
   \item Some word formation processes:
    \begin{itemize}
     \item Compounding - combining free morphemes to make complex words (e.g. dishwasher)
     \item Derivation - combining a base morpheme with a derivational affix (e.g. un-tie, fear-less)
    \end{itemize}
  \end{itemize}
\end{itemize}
- How to distinguish between inflection and derivation
\begin{itemize}
 \item Category change - inflection doesnt change POS, meaning of the word it is applied to, derivation does (thereby creating new words)
 \item Ordering - derivational affixes must combine with the base before inflectional affixes (e.g. dish + washer + s)
 \item Productivity - inflectional affixes are more productive (i.e. easily apply new appropriate bases)
\end{itemize}
- How are morphemes recognized ({\bf Morpheme-based} vs {\bf Lexeme-based} models)
\begin{itemize}
 \item Morpheme-based models
  \begin{itemize}
   \item basic morphological building block is the morpheme
   \item both free and bound morphemes stores in the lexicon with their meaning and grammatical category
   \item complex words generated by concatenation
  \end{itemize}
 \item Lexeme-based models
  \begin{itemize}
   \item basic morphological building block is the lexeme
   \item bound morphemes are not stored in the lexicon, but rather as part of lexeme based morphological rules (e.g. $[X]_v \rightarrow [[X]_v er]_n$ one who does X ) 
   \item motivated by the existence of non-concatenative morphology
  \end{itemize}
\end{itemize}
In the lexicon: For each item: Semantic, categorial, subcategorial, phonological information.\\
Identifying Parts of speech: Notational (semantic), Distributional (syntactic), Formal (morphology.)\\
Closed vs. Open classes.
EXERCISE QUESTION:

\begin{enumerate}
 \item For the following list of words, 1) give the number of morphemes, 2) number of syllables, 3) identify the root, 4) identify the derivational morpheme(s), 5) identify the inflectional morphemes (suffixes): only, unpacked, graphically, bookshops, healthier, disappearing, coldest, proven, John's, mispronounces, actors, fineger
 \item Give an example of a portmanteau morpheme in your native language (ANSWER: -s as in walk-s - morpheme for third-person, singular, and present tense)
\end{enumerate}

\subsection{Linguistics Foundations III - Syntax}
\begin{itemize}
 \item Transformational
  \begin{itemize}
   \item The ultimate goal of ling. theory is to model language acquisition and linguistic competence
   \item Developed from Chomskys' assumption that a theory of grammar is a moel of the mental representation of linguisitic knowledge
   \item Developes in several stages:
    \begin{itemize}
     \item Standard theory (and its extensions and revisions)
      \begin{itemize}
       \item Three basic ingredients:
	\begin{enumerate}
	 \item Phrase structure rules - used to break down a sentence into its constituent parts (i.e. syntactic categories)
	 \item Transformational rules - moving, inserting or deleting constituents and compining phrase markers in oder to preserve meaning
	 \item The Lexico - contains information about lexical items (e.g. phonological, categorial, semantic info...)
	\end{enumerate}
       \item PS Rules + Lexicon $\rightarrow$ Deep Structure $\rightarrow$ Transformational rules $\rightarrow$ Surface Structure
       \item Deep structure - represents the core semantic representation of a sentence
       \item Surface structure
	\begin{enumerate}
	 \item mapped from the deep structure via transformational rules, the surface structure tree's terminal yield is then a grammatical sentence in the language in question. 
	 \item Unlike deep structure, the surface structure trees generated by the transformational rules may represent passive, interrogative, and inflected sentences.
	 \item Different surface structures convey the same meaning when they share the same underlying deep structure
	 \item A surface sentence conveys more than one meaning wen it is associated with more than one deep structure
	 \item The base component (i.e. PS rules and the syntactic categories on which they operate to generate deep structures of sentences) is universal
	 \item The diversity of languages is due to the idiosyncracies of the lexicon and transformational rules for a given language
        \end{enumerate}
       \item Two weak points:
	\begin{enumerate}
	  \item the 'universal` syntactic categories were too specific to be universal (e.g. not all languages have prepositions) and not specific enough to categorize all lexical items of a language (e.g. 'so' and 'there' dont belong to any categories hypothesized as universal)
	  \item Transformational rule turned out too powerful allowing change meanings as in e.g. `Everyone in this room speaks two languages.' $\rightarrow$ 'Two languages are spoken by everyone in this room.'
	  \end{enumerate}
       \item ...Led to a desire to restrict transformational rules via: GB/PPT and Non-Transformational models (e.g. LFG, HPSG,...)
      \end{itemize}     
     \item Government and Binding / Principles and Parameters Theory $\rightarrow$ Minmilast program
     \begin{itemize}
      \item Principles - Universal grammar rules (e.g. A sentence must have a subject)
      \item Parameters - Markers or switches that are turned on/off for individual languages (e.g. head-final parameter off for engllish)
     \end{itemize}
     \item key idea: It is no longer syntactic categories that are claimed to be universal, but rather features
    \end{itemize}
  \end{itemize}
 \item Non-transformational (lexicalist)
  \begin{itemize}
   \item A ling. theory must be descriptively adequate, computationally implementable and/or psychollogically plausible
   \item example: LFG and HPSG are highly lexical non-transformational grammars in which grammaticality is determined by satisfaction of simultaneous constraints (i.e. constraint-based)
    \begin{itemize}
     \item LFG
      \begin{itemize}
       \item Lexical component able to account for transformational phenomena
       \item c-structure: phrase-structure tree serving as the basis for phonological interpretation
       \item f-structure: hierarchical attribute-value matrix representing underlying grammatical relations (SUBJ, OBJ) and serving as the basis for semantic interpretation
      \end{itemize}
     \item HPSG
      \begin{itemize}
       \item Seek to integrate syntax and semantics
       \item A system of signs (words, phrases, sentences) which are related to the signified (meaning)
       \item Signs are represented in feature structures consisting of attributes and values
      \end{itemize}
    \end{itemize}
  \end{itemize}
 \item Cognitive 
  \begin{itemize}
   \item Language is the product of more general c0gnitive abilities and a ling. theory should describe how these cognitive abilities ae used in ling. behavior
   \item Semantics, rather than syntax is the basis of linguistic analysis
   \item Example: Cognitive Grammar
    \begin{itemize}
      \item Basic unit of grammar is 'symbolic assembly' which associates semantic and phonological representations
      \item No reference to syntactic categories
    \end{itemize}
  \end{itemize}
\end{itemize}
- Descriptive vs. Explanatory adequacy
\begin{itemize}
 \item Descriptive - Theory's rules account for all observed data, rules produce all and only the well-formed constructs of the grammar
 \item Explanatory - Theory provides a principled choice between competing descriptions and deals with the uttermost underlying structure
\end{itemize}


\section{ Cognitive Foundations (Crocker) 14.11-18.11}
\subsection{Cognitive Foundations I}
\begin{itemise}
 \item Cognitive Linguistics: First language aquisition, use of language, evolution of language.
 \item Nature vs. Nurture:
 \begin{itemise}
	\item Nature (Chomsky): Language-specific genetic makeup, explains common features, aquisition despite poverty of stimulus.
	\item Nurture (Emergentists: Elman, Bates): solely from experience, language adopted to be adaptable, simpler.
 \end{itemise}
 \item UG: parameters that are set, coincides with language localisation in Brain.
 \item Poverty of stimulus: language unlearnable from evidence alone
 \item Children ignore negative evidence
 \item Creoles and NSL taken as signs of growth despite poverty of stimulus.
 \item Language Acquisition Device: takes data, makes competent grammar.
 \item Challenging Nativism: Most people believe in some innateness, poverty of stimulus overstated, sophisticated statistical techniques possible.
 \item Language relativity (Sapri, Whorf, Lakoff, Levinson): Language influences thought (culture)
 \item Linguistic autonomy (Chomsky, Pinker, Fodor): We all have knowledge of language, different from thought.
 \item Geocentric languages overview: Pompurra.
 \item Modularity in the brain: Does language exist separately or not? We're not sure.
 \item Distinct: Representational (storage) vs. procedural (access)  autonomy. 
\end{itemise}


%\section{Technological Foundations (Busemann) 21.11-25.11}
%
%\section{Finite State Methods for Lexicon \& Morphology (Kiefer) 28.11-02.12}
%
\section{Parsing (Zhang) 05.12-09.12}

- Bottom-up vs. Top-Down parsing
\begin{itemize}
 \item Bottom-up - Start from input sequence and apply grammar rules to build tree upwards
 \item Top-down - Start from the start symbol and expand the tree with grammar rules
\end{itemize}
- CKY Algorithm
\begin{itemize}
 \item A bottom-up chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$ 
 \item Grammar must be in CNF
 \item Online javascript example: http://www.diotavelli.net/people/void/demos/cky.html
\end{itemize}
- Earley Algorithm
\begin{itemize}
 \item Top-Down chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$
 \item Example (pdf, slides 18-36): http://www.coli.uni-saarland.de/~yzhang/rapt-ws1112/slides/schmidt.pdf
\end{itemize}
- Inside Outside Algorithm
\begin{itemize}
 \item 
\end{itemize}


%\section{Statistical NLP (Language Models) (Klakow, Wiegand) 12.12-23.12}
%
%\section{Prosodic Models for Speech Technology (Moebius) 09.01}
%
%\section{Speech Synthesis (Moebius, Lasarcyk) 11.01-16.01}
%
%\section{Automatic Speech Recognition (Moebius) 18.01}
%
%\section{Corpora for speech technology (Lasarcyk) 20.01}
%
%\section{Semantics (Pinkal) 23.01-3.02}
%
%\section{Discourse \& Dialogue (Sporleder) 06.02-08.02}


\end{document}