%\requirepackage{lineno}
\documentclass[11pt]{article}
\usepackage[left=4cm,top=3cm,right=3cm,left=3cm,nofoot]{geometry}  \geometry{a4paper}                   
\usepackage{tipa, apalike, graphicx, amssymb, delarray, epstopdf, amsmath, amsthm, setspace, supertabular, qtree, hyperref, footnote, palatino, url, multicol, hanging, fullpage, supertabular, ulem} %, lineno, gb4e} 
%gb4e messes up math mode. 
\clubpenalty=300 \widowpenalty=300
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%\usepackage[parfill]{parskip} 			%Activate for line, not indent, paragraphs.
\usepackage{ stmaryrd }
\newenvironment{itemise}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}


\title{FSLT Examination Preparation}
\author{Richard Littauer, Lamar Payson }		% Add your name here
\date{\today}                          		% Activate to display a given date or no date
\singlespace
\begin{document}
\maketitle
\tableofcontents

\section*{Overview}
%The point is to have a latex analysis of the slides or relevant facts to know for each section, and subsequently, each slide. We can work on this collaboratively - remember to push and pull the sections you're working on, and try to make sure that you aren't clashing with other's doing the same work. 
Format of the exam:
\begin{itemise}
\item 9 short questions for each section. These are obligatory. 
\item 5 long questions from 6 sections. You can pick up 3 from them. 
\begin{itemise}
\item linguistic foundations
\item finite-state automata
\item parsing
\item seminar
\item statistical NLP
\item speech
\end{itemise}
\end{itemise}

\section{Map of the Field (Uzkoreit) 24.10-26.10}

Was anything actually covered in these two sessions? Does anyone remember? 

\newpage\section{Linguistics Foundation (Delogu) 28.10-09.11}
\subsection{Linguistic Foundations I}
% By Lamar
Properties of human language: Arbitrary (no relation), Discrete (units), Duality (sounds, meanings), Displacement (things not here), Creative (infinitely so).

- The difference between linguistic competence and linguistic performance
\begin{itemise}
 \item \textbf{Linguistic competence} -The implicit knowledge a language user has of his language, which enables him to produce and understand any possible sentence of that language
 \item \textbf{Linguistic performance} - The actual use of language in real situations, which is conditioned by physiological and psychological constraints (memory limitations, shifts of attention, etc.)
\end{itemise}
- The difference between sentence gramaticality and acceptability
\begin{itemise}
 \item \textbf{Gramaticality} - A sentence is gramatical if it is formed according to the gramar of the language
\item \textbf{Acceptability} - A sentence is acceptable if it 'sounds good' to a native speaker
\end{itemise}
note: a sentence can be gramatical but unnacceptale (e.g. hard to process)
\\\\
- Four definititions of \textbf{grammar}
\begin{itemise}
 \item The linguistic rules of a language that every native speaker intuitevly knows - Linguistic competence
\item A model of the linguistic rules that every speaker of a language intuitively knows - A theory of linguistic competence 
\item The rules and principles that describe the linguistic behavior of native speakers - Descriptive grammar
\item - The rules and principles that prescribe the linguistic behavior of native speakers, according to some authority - Prescreptive grammar
\end{itemise}
- Linguistic structuralism and the \textbf{inductive method} 
\begin{itemise}
 \item Structural linguistics thus involves collecting a corpus of utterances and then attempting to classify all of the elements of the corpus at their different linguistic levels
\item linguistic constituents were identified by the set of all contexts in which they can occur
\end{itemise}
- Chomsky and the \textbf{Generative Program}
\begin{itemise}
\item A rejection of structuralism and a redefinition of the object of investigation
\item Developed from two key observations:
  \begin{enumerate}
  \item Linguistic competence: people are able to understand and produce an infinite number of grammatical sentences
    \begin{itemise}
    \item The inductive method used by the Structuralists was inadequate (and unable to account for key linguistic phenomena like structural ambiguity)
    \end{itemise}
  \item Language acquisition: children are able to ‘learn’ their language perfectly, even though they are exposed to defective inputs
    \begin{itemise}
    \item The idea that language is learnt through stimulus-response processes, as argued by the Behaviorists (e.g., Skinner 1957) was no longer tenable
    \end{itemise}
  \end{enumerate}
\item The task of a linguist is comparable to a child’s acquisition of linguistic competence.
  \begin{itemise}
  \item Since children do not acquire language inductively, the new methodology must be deductive
  \end{itemise}
\item The object of investigation is no longer the product language as represented by corpora but the linguistic competence of native speakers
\item Linguistic competence is an internalized (mentally- represented) grammar capable of generating an infinite number of grammatical sentences and none of the ungrammatical ones
\item The goal of linguistics is to provide an adequate grammar as a model of linguistic competence
\item Levels of adequacy: Observational, descriptive, explanatory. 
\item Generative Grammar: grammars must be precise, tested against invented data, must be a theory of human competence. 
\item Controversial chomskyan claims: all languages are similar, as linguistic knowledge is innate, and the central problem is acquistion. 
\item GG: Standard, Principles and Parameters, Government and Binding, X-Bar, Miminalism.
\item Non-GG: LFG, HPSG, TAG, CG, Functional, Cognitive...
\end{itemise}

\subsection{Linguistic Foundations II - Learn you a morphology}
% By Lamar
\begin{itemise}
 \item \textbf{Morphology} - The study of word structure and word formation
\end{itemise}

- What is a word?
\begin{itemise}
 \item \textbf{Lexeme} - An abstract decontextualized vocabulary item with a core meaning (e.g. {\it see})
 \item \textbf{Word form} - A particular physical realization of a lexeme in text or writting (e.g. {\it sees, saw, seen}) 
 \item \textbf{Grammatical word} - A representation of a lexeme that is associated with certain morpho-syntactic properties
\end{itemise}
- What is a morpheme
\begin{itemise}
 \item {\bf Morpheme} - The minimal, indivisible units of semantic content or grammatical function which words are made up of
  \begin{itemise}
   \item e.g. printable $\rightarrow$ print + able
  \end{itemise}
  \item Morphemes are made manifested in a sequence of sounds called {\bf morphs}
  \begin{itemise}
    %I'm too lazy to figure out how to do ipa symbols
    \item Different pairings between a morpheme and morphs is possible (i.e. homophones e.g. /sait/ $\rightarrow$ site, cite, sight and allomorphs e.g. 'past-tense marker' $\rightarrow$ /id/, /d/, /t/)  
  \end{itemise}
\end{itemise}
- Types of morphemes
\begin{itemise}
 \item Root - Carry a lexical meaning and usually belong to a lexical category
 \item Inflectional - Carry gramatical meaning (e.g. case, number, person, gender, etc.)
 \item Derivational - Serve to form complex words and can change the lexical category of the word it is attached to (e.g. -able $/rightarrow$ do + able $/rightarrow$ doable, verb to adjective)
 \item Free - Can stand alone, don't need to be attached
  \begin{itemise}
    \item Open class - Content words (nouns, adjectives, verbs, adverbs)
    \item Closed class - Function words  (conjunctions, prepositions, articles, pronouns, auxiliaries)
  \end{itemise}
 \item Bound - have to be attached to another morpheme (e.g. suffixes)
\end{itemise}
Morphemes have allomorphs, which are either phonologically (s/z), grammatically (weep/wept), or lexically conditioned (ox/oxen).
- {\bf Portmanteau morphemes}
\begin{itemise}
 \item a morph that represents several morphemes (e.g. walk-{\bf s} where -s is a morpheme for third person, present tense, and singular)
\end{itemise}
- {\bf Inflectional} vs. {\bf Derivational} morphology
\begin{itemise}
 \item Inflectional - determines the appropriate form of a morpheme in a given context
  \begin{itemise}
   \item Marks grammatical distinctions (e.g. singular/plural) 
   \item Is required by syntactic criteria (e.g. English nouns are required to have number)
   \item Some inflectional processes:
    \begin{itemise}
     \item Affixation - Addition of an inflectional affix (e.g. -s plural marker)
     \item Internal change - Substitution of one nonmorphemic segment with another to mark a grammatical contrast
     \item Suppletion - Replacement of a morpheme with an entirely different morpheme to mark grammatical contrast (e.g. forms of be - am, is, are, were)
    \end{itemise}
  \end{itemise}
 \item Derivational (word formation) - Creates complex words by joining free and bound morphemes 
  \begin{itemise}
   \item Are opitional, i.e. not required by syntactic criteria
   \item Some word formation processes:
    \begin{itemise}
     \item Compounding - combining free morphemes to make complex words (e.g. dishwasher)
     \item Derivation - combining a base morpheme with a derivational affix (e.g. un-tie, fear-less)
    \end{itemise}
  \end{itemise}
\end{itemise}
- How to distinguish between inflection and derivation
\begin{itemise}
 \item Category change - inflection doesnt change POS, meaning of the word it is applied to, derivation does (thereby creating new words)
 \item Ordering - derivational affixes must combine with the base before inflectional affixes (e.g. dish + washer + s)
 \item Productivity - inflectional affixes are more productive (i.e. easily apply new appropriate bases)
\end{itemise}
- How are morphemes recognized ({\bf Morpheme-based} vs {\bf Lexeme-based} models)
\begin{itemise}
 \item Morpheme-based models
  \begin{itemise}
   \item basic morphological building block is the morpheme
   \item both free and bound morphemes stores in the lexicon with their meaning and grammatical category
   \item complex words generated by concatenation
  \end{itemise}
 \item Lexeme-based models
  \begin{itemise}
   \item basic morphological building block is the lexeme
   \item bound morphemes are not stored in the lexicon, but rather as part of lexeme based morphological rules (e.g. $[X]_v \rightarrow [[X]_v er]_n$ one who does X ) 
   \item motivated by the existence of non-concatenative morphology
  \end{itemise}
\end{itemise}
In the lexicon: For each item: Semantic, categorial, subcategorial, phonological information.\\
Identifying Parts of speech: Notational (semantic), Distributional (syntactic), Formal (morphology.)\\
Closed vs. Open classes.
EXERCISE QUESTION:

\begin{enumerate}
 \item For the following list of words, 1) give the number of morphemes, 2) number of syllables, 3) identify the root, 4) identify the derivational morpheme(s), 5) identify the inflectional morphemes (suffixes): only, unpacked, graphically, bookshops, healthier, disappearing, coldest, proven, John's, mispronounces, actors, fineger
 \item Give an example of a portmanteau morpheme in your native language (ANSWER: -s as in walk-s - morpheme for third-person, singular, and present tense)
\end{enumerate}

\subsection{Linguistics Foundations III - Syntax}
% By Lamar
\begin{itemise}
 \item {\bf Transformational}
  \begin{itemise}
   \item The ultimate goal of ling. theory is to model language acquisition and linguistic competence
   \item Developed from Chomskys' assumption that a theory of grammar is a moel of the mental representation of linguisitic knowledge
   \item Developes in several stages:
    \begin{itemise}
     \item {\bf Standard theory} (and its extensions and revisions)
      \begin{itemise}
       \item Three basic ingredients:
	\begin{enumerate}
	 \item {\bf Phrase structure rules} - used to break down a sentence into its constituent parts (i.e. syntactic categories)
	 \item {\bf Transformational rules} - moving, inserting or deleting constituents and compining phrase markers in oder to preserve meaning
	 \item {\bf The Lexicon} - contains information about lexical items (e.g. phonological, categorial, semantic info...)
	\end{enumerate}
       \item PS Rules + Lexicon $\rightarrow$ Deep Structure $\rightarrow$ Transformational rules $\rightarrow$ Surface Structure
       \item {\bf Deep structure} - represents the core semantic representation of a sentence
       \item {\bf Surface structure}
	\begin{enumerate}
	 \item mapped from the deep structure via transformational rules, the surface structure tree's terminal yield is then a grammatical sentence in the language in question. 
	 \item Unlike deep structure, the surface structure trees generated by the transformational rules may represent passive, interrogative, and inflected sentences.
	 \item Different surface structures convey the same meaning when they share the same underlying deep structure
	 \item A surface sentence conveys more than one meaning wen it is associated with more than one deep structure
	 \item The base component (i.e. PS rules and the syntactic categories on which they operate to generate deep structures of sentences) is universal
	 \item The diversity of languages is due to the idiosyncracies of the lexicon and transformational rules for a given language
        \end{enumerate}
       \item Two weak points:
	\begin{enumerate}
	  \item the 'universal` syntactic categories were too specific to be universal (e.g. not all languages have prepositions) and not specific enough to categorize all lexical items of a language (e.g. 'so' and 'there' dont belong to any categories hypothesized as universal)
	  \item Transformational rule turned out too powerful allowing change meanings as in e.g. `Everyone in this room speaks two languages.' $\rightarrow$ 'Two languages are spoken by everyone in this room.'
	  \end{enumerate}
       \item ...Led to a desire to restrict transformational rules via: GB/PPT and Non-Transformational models (e.g. LFG, HPSG,...)
      \end{itemise}     
     \item {\bf Government and Binding / Principles and Parameters Theory} $\rightarrow$ {\bf Minmilast program}
     \begin{itemise}
      \item {\bf Principles} - Universal grammar rules (e.g. A sentence must have a subject)
      \item {\bf Parameters} - Markers or switches that are turned on/off for individual languages (e.g. head-final parameter off for engllish)
     \end{itemise}
     \item key idea: It is no longer syntactic categories that are claimed to be universal, but rather features
    \end{itemise}
  \end{itemise}
 \item {\bf Non-transformational} (lexicalist)
  \begin{itemise}
   \item A ling. theory must be descriptively adequate, computationally implementable and/or psychollogically plausible
   \item example: LFG and HPSG are highly lexical non-transformational grammars in which grammaticality is determined by satisfaction of simultaneous constraints (i.e. constraint-based)
    \begin{itemise}
     \item {\bf LFG}
      \begin{itemise}
       \item {\bf Lexical component} able to account for transformational phenomena
       \item {\bf c-structure}: phrase-structure tree serving as the basis for phonological interpretation
       \item {\bf f-structure}: hierarchical attribute-value matrix representing underlying grammatical relations (SUBJ, OBJ) and serving as the basis for semantic interpretation
      \end{itemise}
     \item {\bf HPSG}
      \begin{itemise}
       \item Seek to integrate syntax and semantics
       \item A system of signs (words, phrases, sentences) which are related to the signified (meaning)
       \item Signs are represented in feature structures consisting of attributes and values
      \end{itemise}
    \end{itemise}
  \end{itemise}
 \item {\bf Cognitive} 
  \begin{itemise}
   \item Language is the product of more general cognitive abilities and a ling. theory should describe how these cognitive abilities ae used in ling. behavior
   \item Semantics, rather than syntax is the basis of linguistic analysis
   \item Example: {\bf Cognitive Grammar}
    \begin{itemise}
      \item Basic unit of grammar is {\bf symbolic assembly} which associates semantic and phonological representations
      \item No reference to syntactic categories
    \end{itemise}
  \end{itemise}
\end{itemise}
- {\bf Descriptive} vs. {\bf Explanatory} adequacy
\begin{itemise}
 \item {\bf Descriptive} - Theory's rules account for all observed data, rules produce all and only the well-formed constructs of the grammar
 \item {\bf Explanatory} - Theory provides a principled choice between competing descriptions and deals with the uttermost underlying structure
\end{itemise}

\subsection{Linguistic Foundations IV - Semantics}
% By Lamar
- Whats all this, then?
\begin{itemise}
 \item Meaning can be seen as the relation between linguistic signs (words, phrases, sentences) and the things they represent (concepts, things in the world, situations, etc.)
 \item Linguistic meaning can be investigated via: 
  \begin{itemise}
   \item Semantic relations between words
    \begin{itemise}
     \item E.g. {\bf Lexical semantics} - the study of lexical meaning
     \item The relationship between words and meanings is arbitrary
      \begin{itemise}
       \item Same word, diff meaning - Homonomy vs. polysemy
       \item Diff. words, same meaning - Synonymy
       \item {\bf Homonymy} - Two words that sound the same but have diff. meanings (e.g. 'bank' - financial institution OR side of a river
       \item {\bf Polysemy} - One word, multiple related meanings (e.g. 'bank' - financial instution OR building housing a financial institution) 
       \item {\bf Qualia roles} - Account for the multiple ways we can represent an object (e.g. Constitutive, Agentitive, Telic)
       \item Types of {\bf antonyms}:
	\begin{enumerate}
	 \item {\bf Complementary pairs} - Presence of one signifies the absence of the other (e.g. married/single)
	 \item {\bf Gradable pairs} - Gradual transition between poles (e.g. hot/cold)
	 \item {\bf Relational pairs} - Same situation from opposite points of view (e.g. buyer/seller)
	\end{enumerate}
       \item {\bf Hyponym} - Words naming subclasses (is-a) (e.g. 'Terrier' is a hyponym for 'dog')
       \item {\bf Meronym} - Words meaning parts (has-a) (e.g. Finger is meronym of hand) 
      \end{itemise}
    \end{itemise}
   \item Semantic relations between sentences
    \begin{itemise}
     \item E.g. {\bf Truth-conditional semantics} - We know the meaning of a sentence when we know what the world must be like for that sentence to be true (i.e. {\bf truth-conditions})
     \item Relations between sentences:
      \begin{itemise}
       \item {\bf Entailment}
	\begin{enumerate}
	 \item A entails B iff whenever A is true, B is true (e.g. `I have 2 children' $\models$ 'I have 1 child')
	 \item Can be extended to the hyponym/hypernym relation (e.g. 'I have a terrier' $\models$ 'I have a dog')
	\end{enumerate}
       \item {\bf Equivalence}
	\begin{enumerate}
	 \item A and B are equivalent iff A entails B and B entails A
	 \item works with synonymous words (e.g. 'A died' is equivalent to 'A passed away')
	\end{enumerate}
       \item {\bf Contradiction}
	\begin{enumerate}
	 \item A and B are in contradiction if they cannot be true at the same time
         \item works with antonyms (e.g. 'John is married' is in contradiction to 'John is single')
	\end{enumerate}
       \item {\bf Presupposition}
	\begin{enumerate}
	 \item B pressuposes A if for B to be true, A must be true
	 \item e.g. 'John has quit smoking' presupposes `John was a smoker'
	 \item Presupposition survives the {\bf negation test} (e.g.John has not quit smoking still presupposes 'John was a smoker')
	 \item NOTE: this is not the case for entailment
	 \item Presupposition triggers:
	  \begin{enumerate}
	   \item Lexical triggers: (regret, realize, know, stop, start, blame, fault)
	   \item Cleft constructions: (It was John that ate the sandwhich -> Someone ate the sandwhich)
	  \end{enumerate}
	 \item Words can have presuppositions (A sleeps -> A is capble of sleeping [c.f. green ideas sleep furiously])
	\end{enumerate}
       \item {\bf Implicature}
	\begin{enumerate}
	 \item A implicates B if A suggests B is true without entailing it
	 \item e.g. 'Some students passed' implies 'Not all students passed'
         \item ...but implicatures are cancleable
	 \item e.g. 'Some students passed, in fact all students passed.'
	\end{enumerate}
      \end{itemise}
    \end{itemise}
  \end{itemise}
\end{itemise}
- Ambiguity
\begin{itemise}
 \item 3 types:
  \begin{enumerate}
   \item Lexical
    \begin{itemise}
     \item Word sense ambiguity (e.g. John {\bf made} her duck [made can mean cook, create, etc.])
     \item POS ambiguity (e.g. John made {\bf her duck} [her may be personal or possessive pronoun])
    \end{itemise}
   \item Structural - When a part of a sentnece may be interpreted as having different structures (e.g. She saw the man with the binoculars)
   \item Scope - e.g. Everyone loves someone
  \end{enumerate}

\end{itemise}

%===========================================================================

\newpage\section{ Cognitive Foundations (Crocker) 14.11-18.11}
\subsection{Cognitive Foundations I}
\begin{itemise}
 \item Cognitive Linguistics: First language aquisition, use of language, evolution of language.
 \item Nature vs. Nurture:
 \begin{itemise}
	\item Nature (Chomsky): Language-specific genetic makeup, explains common features, aquisition despite poverty of stimulus.
	\item Nurture (Emergentists: Elman, Bates): solely from experience, language adopted to be adaptable, simpler.
 \end{itemise}
 \item UG: parameters that are set, coincides with language localisation in Brain.
 \item Poverty of stimulus: language unlearnable from evidence alone
 \item Children ignore negative evidence
 \item Creoles and NSL taken as signs of growth despite poverty of stimulus.
 \item Language Acquisition Device: takes data, makes competent grammar.
 \item Challenging Nativism: Most people believe in some innateness, poverty of stimulus overstated, sophisticated statistical techniques possible.
 \item Language relativity (Sapri, Whorf, Lakoff, Levinson): Language influences thought (culture)
 \item Linguistic autonomy (Chomsky, Pinker, Fodor): We all have knowledge of language, different from thought.
 \item Geocentric languages overview: Pompurra.
 \item Modularity in the brain: Does language exist separately or not? We're not sure.
 \item Distinct: Representational (storage) vs. procedural (access)  autonomy.
 \item Views of human perception:
 \begin{itemise} 
 \item Cognitivist: inferential, unencapsulated. 
 \item Behaviouralist: non-inferential, encapsulated.
 \item Fodor: inferential but encapsulated. Modules are:
 \begin{itemise}
 \item domain specific, innately specified, informationally encapsulated, fast, hardwired, autonomous, not assembled.
 \item Three types: transducers (physical to neural), input system (modular), central system.
 \end{itemise}
 \end{itemise}
 \item Language in the brain:
 \begin{itemise}
 \item Broca's area (frontal lobe): language production/comprehension
 \item Wernicke's area (temporal lobe): language processing
 \item Occipital lobe: visual processing.
 \end{itemise}
 \item Proof of modularity:
  \begin{itemise}
  \item Double dissasociation: cognitive functions without language, vice versa.
  \item Broca's aphasia, language impairment: production is impaired
  \item Williams syndrome, senile dementia: diminished memory, functioning, language fine.
 \end{itemise}
 \item Second language test found activation only for UG grammars.
 \item Foxp2: KE family: speech disorder, under-fireing of Broca's area, issues with comprehension.
 \item Language evolution: on an individual, cultural, and biological feedback level.
 \item Symbol grounding problem: how are perceptual states mapped to/from amodal symbols. 
 \item Perceptual grounding: mental representations of words run through (mental) simulators based on perceptual and motor experience schemas. 
 \item Backed up by the action compatibility effect, involving perceived actions with mental actions (turning, etc.)
 \end{itemise}

\subsection{Cognitive Foundations II}
 \begin{itemise}
 \item Competence: How do utterances relate to underlying language?
 \item Performance: How do people relate this during on line language processing?
 \item Production issues: Spoonerisms, agreement mismatches.
 \item Comprehension issues: centre embedding, garden paths
 \item Language is understood incrementally, and we've got to decode local and global ambiguity.
 \item Reduced Relative Clauses: The man delivered the junk mail threw it away.
 \item Parsers make mistakes, too: in some cases, can recover from earlier error.
 \item Reading times can be used to investigate ambiguities. Methods: whole sentence, self-paced central, self-paced moving.
 \item Neuroscientific methods: syntactosemantic processes revealed by EEG. Examples include semantic anomalies, lexical access over time, anticipation
 \item Cognitive models: Constrained and Unconstrained:
  \begin{itemise}
  \item C: Emphases on cognitive constraints, like memory/modularity
  \item C: Evidence from different structures, ambiguities, pathologies
  \item C: Methods proposed optimised for these
  \item UC: Emphasis on fast nature of brain
  \item UC: Mechanisms emphasize optimal use of information.
 \end{itemise}
 \item Two main theories of human parsing: Frazier, McRae
  \begin{itemise}
  \item F: serial parsing with reanalysis for constructing interpretations
  \item F: generic syntactic principles for most likely structure
  \item M: Competitive activation of alternatives in parellel
  \item M: Competitive integration of constraints
 \end{itemise}
 \item A theory of parsing must specify: mechanism, information sources, representation preferred in case of ambiguity.
 \item Linking Hypothesis: Relate the theory or model to some observed measure.
 \item Fodor represented language as a module, but what about language-internal levels?
 \item Frazier: Parsing: serial, reanalyse based on syntactic conflict, thematic fit.
 \item Frazier Garden path parsing: only syntactic considerations, takes into account memory constraints, strategy universal and innate.
 \item Two strategies: minimal attachment, or late closure.
 \item Garden Path Theory (Frazier):
 \begin{itemise}
\item What architecture is assumed? -  Modular syntactic processor, with restricted lexical (category)
and semantic knowledge 
\item What mechanisms is used to construct interpretations? - Incremental, serial parsing, with reanalysis
\item What information is used to determine preferred structure? - General syntactic principles based on the current phrase structure
\item Linking Hypothesis:  - Parse complexity and reanalysis cause increased RTs
 \end{itemise}
 \item Arguments against it: immediate effects (animacy, frequency, plausability, context), appropriate computational frameworks, integrative theories (HPSG) with multiple representaitons in a unified formalism.
 \item The Competitive Integration Model (McRae)
 \begin{itemise}
 \item Claim: Diverse constraints (linguistic, conceptual) influence ambiguity resolution.
 \item Model: Make all possible analyses, choose from among them.
 \item Model: {\it Cats Don't Wear Cell Phones}.
 \begin{itemise}
 \item 1. {\bf Combine} constraints as they become available for input.
 \item 2. Input {\bf determines} the probabilistic activation for each constraint.
 \item 3. {\bf Weigh} constraints according to strength.
 \item 4. Alternatives {\bf compete}.
 \item 5. {\bf Map} cycles to reading time.  
 \end{itemise}
 \item Possible constraints: By-constraint, thematic role, verb tense, main clause...
 \item Constraint based models:
 \begin{itemise}
 \item non-modular architecture assumed.
 \item parallel construction based on constraint activations
 \item all information used at once
 \item comprehension easy when common interpretation, difficult otherwise.
 \end{itemise}
 \item A good model:
 \begin{itemise}
 \item should make independently motivated assumptions
 \item clearly defined parameters
 \item clear linking hypothesis to empirical methods
 \item predict unseen data
 \end{itemise}
\end{itemise}
\end{itemise}

%===========================================================================

\newpage\section{Technological Foundations}
% By Lamar
\begin{itemise}
 \item Text Technologies:
\begin{itemise}
 \item gathering
 \item indexing
 \item categorization
 \item clustering
 \item summarization
\end{itemise}
 \item Speech Technologies:
\begin{itemise}
 \item Voice Recognition
 \item Speech Synthesis
 \item Speakier identification
\end{itemise}
 \item Language Technologies
\begin{itemise}
 \item Machine translation
 \item Language understanding
 \item Language generation
\end{itemise}
\end{itemise}
- Some technologies (a bit more in depth)
\begin{enumerate}
 \item Speech recognition
\begin{itemise}
 \item Spoken language is recognized and transformed into:
\begin{itemise}
 \item text (dictation system)
 \item commands (robot control system)
\end{itemise}
\end{itemise}
 \item Speech Synthesis
\begin{itemise}
 \item Utterances in spoken language are produced from 
\begin{itemise}
 \item text (text-to-speech systems)
 \item internal representations of words or sentences (concept-to-speech systems)
\end{itemise}
\end{itemise}
 \item Text Categorization
\begin{itemise}
 \item Texts are assigned to given categories.
 \item Filtering is a special case of categorization with just two categories
\end{itemise}
 \item Text Summarization
\begin{itemise}
 \item The most relevant portions of a text are extracted as a summary.
\end{itemise}
 \item Text Indexing
\begin{itemise}
 \item A precondition for document retrieval
 \item Texts are stored in an indexed database.
\end{itemise}
 \item Text Retrieval
\begin{itemise}
 \item Texts that best match a given query or document are retrieved from a database
 \item Candidate documents ordered with respect to their expected relevance 
\end{itemise}
 \item Information Extraction
\begin{itemise}
 \item Relevant pieces of information are discovered and marked for extraction
 \item e.g. topic, named entities, simple relations such as prices, etc.
\end{itemise}
 \item Data Fusion and Text Data Mining
\begin{itemise}
 \item Extracted pieces of information from several sources are combined into one database.
\end{itemise}
 \item Question Answering
\begin{itemise}
 \item Natural language queries are used to access information in a database
\end{itemise}
 \item Report Generation
\begin{itemise}
 \item A report in natural language is produced that describes the requested contents or changes of a database.
\end{itemise}
 \item Spoken Dialogue Systems
\begin{itemise}
 \item System carries out a dialogue with a human user in which the user can solicit information, conduct purchases, etc.
\end{itemise}
 \item Translation Technologies
\begin{itemise}
 \item Texts are translated autmatically, or the system assists a human translator.
\end{itemise}
\end{enumerate}
- Formal and Computational Methods
\begin{itemise}
 \item Generic Computer Science Methods
\begin{itemise}
 \item Programming languages
 \item Algorithms for generic data tyoes
 \item Software engineering methods
\end{itemise}
 \item Specialized Algorithms
\begin{itemise}
 \item Dedicated algorithms for parsing, generation, translation, etc.
\end{itemise}
 \item Non-discrete Mathematical Methods
\begin{itemise}
 \item Statistical methods
 \item Neural networks
\end{itemise}
 \item Logical and Linguistic Formalisms
\begin{itemise}
 \item Constraint-based grammar formalisms
 \item Formalisms for representation of semantic content
\end{itemise}
 \item Linguistic Knowledge
\begin{itemise}
 \item Dictionaries
 \item Morphological and syntactic grammars
 \item Pronunciation rules
 \item etc.
\end{itemise}
 \item Corpora and Corpus tools
\begin{itemise}
 \item Large collections of application-specific or generic spoken and written language sources
\end{itemise}
 \item Models of Cognitive Systems and their cmponents
\begin{itemise}
 \item The interaction of perception, knowledge, reasoning and action (including communication) is modeled in cognitive pyshology
 \item Such models can be consulted or employed for the design of language processing systems.
\end{itemise}
 \item Empirical methods from Experimental Psycology
\begin{itemise}
 \item Methods for the observation and analysis of language production and comprehension taken from the field of cognitive psychology
\end{itemise}
\end{itemise}
- {\bf Competence} vs. {\bf Performance}
\begin{itemise}
 \item Competence: skills and abilities needed to solve a problem. Can not be observed directly.
\begin{itemise}
 \item e.g. wrt language - Competence: people know the grammar of english
\end{itemise}
 \item Performance: behaviour in solving a probelm. Can be observed.
\begin{itemise}
 \item e.g. wrt language - Performance: People produce utterances
\end{itemise}
 \item LT systems make no distinction between competence and performane.
\end{itemise}
- Some difficulties for LT systems
\begin{itemise}
 \item Out-of-domain talk for a two-party dialogue system
 \item LT systems have no mechanism for error anticipation (e.g. preemptive rephrasing) which humans are capable of
\end{itemise}

\newpage\section{Finite State Methods for Lexicon \& Morphology (Kiefer) 28.11-02.12}
% By Lamar
NOTE: I didn't have time to draw a bunch of FSA transition grpahs by hand, so if you're unfimilar with how creating or interpreting transition graphs, you'll have to take a look at the slides.

\subsection{ 1st Lecture }

- Finite Automata: Formal Definition
\begin{itemise}
 \item An FA is a tuple $A=<Q, \Sigma , \delta , q_0 , F>$ where:
  \begin{itemise}
   \item ${\bf Q}$ is a finite non empty {\bf set of states}
   \item ${\bf \Sigma}$ is a finite {\bf alphabet of input letters}
   \item ${\bf \delta}$ is a {\bf transition function} $Q \times \Sigma \rightarrow Q $
   \item ${\bf q_0 \in Q}$ is the {\bf initial state}
   \item ${\bf F \subseteq Q}$ is a set of {\bf final states}
  \end{itemise}
\end{itemise}
- String Acceptance
\begin{itemise}
 \item An FA {\bf accepts} an input string $s$ if there is a sequence of states $p_1 , p_2 , \dotsi , p_{|s|} \in Q$ such that
  \begin{enumerate}
   \item $p_1 = q_0$, the start state
   \item $\delta(p_i , s_i ) = p_{i+1}$, where $s_i$ is the $i$-th character is $s$
   \item $p_{|s|} \in F$, i.e. is a final state
  \end{enumerate}
 \item The set of strings accebted by an automaton is the accepted {\bf language}
\end{itemise}
- Nondetermenistic Automata
\begin{itemise}
 \item $\delta$ is a {\bf transition relation} (i.e. not necessarily a {\bf total function})
 \item $\delta : Q \times \Sigma \cup \{ \epsilon \} \rightarrow \mathcal{P}(Q)$, where $\mathcal{P}(Q)$ is the powerset of $Q$
 \item allows transitions from one state into several possible states with the same inut symbol
 \item can have transitions labeled $\epsilon$ which represent the empty string
 \item for every NFA, an equivalent DFA can be constructed
\end{itemise}
- Converting NFA to DFA via the subset construction algorithm
\begin{itemise}
 \item Main Idea: Simulate `in parallel' all possible moves the automaton can make
 \item Uses 2 operations on states/state-sets of the NFA
 \begin{enumerate}
  \item $\epsilon$-$closure(T)$ : Set of states reachable from any $s$ in $T$ on $\epsilon$-transitions
  \item $move(T, a)$ : Set of states to which there is a transition from state in $T$ on an input symbol $a$
 \end{enumerate}
 \item NOTE: I don't like reading algorithms, and there is a nice example in the slides, that makes it clear without having to read through the pseudocode. I would recommend reading the example. finiteautomat1.pdf slides 16 - 24
\end{itemise}

\subsection{2nd Lecture}
% By Lamar
- A (small) Regular Expression cheat sheet
\begin{center}
\begin{tabular}{|l|l|}\hline
RE & Description\\\hline
a* & 0 or more a's\\\hline
a+ & 1 or more a's\\\hline
a? & 0 or 1 a's\\\hline
cat|dog & 'cat' or 'dog'\\\hline
\^a & an 'a' at the start of a line\\\hline
a\$ & an 'a' at the end of a line\\\hline
[a-zA-Z] & 1 of any UC or LC letter\\\hline
[0-9] & 1 of any digit\\\hline
\textbackslash ba & an 'a' at the start of a word\\\hline
a\textbackslash B & an 'a' followed by the rest of a word\\\hline
\end{tabular}
\end{center}
- Formal languages
\begin{itemise}
 \item Some notation:
  \begin{itemise}
    \item Alphabet ${\bf\Sigma}$ : nonempty finite set of terminal symbols
    \item Alphabet ${\bf N}$ : of non-terminal symbols
    \item ${\bf\epsilon}$ : the empty string
    \item ${\bf\Sigma^*}$ : the set of all strings over $\Sigma$ (including $\epsilon$
    \item ${\bf L}$ a language st. $L \subseteq \Sigma^*$
    \item $w$ a sentence
  \end{itemise}
 \item Chomskyan hierarchy (of language complexity)
  \begin{itemise}
   \item Type 3: Regular languages (NOTE: the focus of this lecture)
    \begin{itemise}
     \item Rule type: $A \rightarrow \alpha, A \rightarrow \alpha B; A,B \in N; \alpha \in \Sigma^*$
     \item Rule system is {\bf right linear}, i.e. nonterminals always on the right end of the production
     \item A linear (in the size of the string) number of steps can determine $w \in L$s
     \item Can't handle $a^n b^n , n \in N$
     \item equivalent to {\bf finite automata}
    \end{itemise}
   \item Type 2: Context free languages
    \begin{itemise}
     \item Rule type: $A \rightarrow \psi; \psi \in (\Sigma \cup N)^*$
    \end{itemise}   
   \item Type 1: Context sensitive languages
    \begin{itemise}
     \item Rule type: $\alpha A \beta \rightarrow \alpha \psi \beta; \alpha, \beta \in \Sigma^*$
    \end{itemise}
   \item Type 0: Unrestricted
    \begin{itemise}
     \item Rule type: $\alpha A \beta \rightarrow \psi$
    \end{itemise}
   \item NOTE: Type 3 $\subset$ Type 2 $\subset$ Type 1 $\subset$ Type 0
  \end{itemise}
\end{itemise}
- Closure properties
\begin{itemise}
 \item Language $A$ is closed under operation $x$ iff applying $x$ to members of $A$ results in elements of the same type.
 \item in plain language e.g.: `If for any two regular languages $A$ and $B$, I combine the two using some operation $x$ (e.g. concatenation) into a new language $C$, will $C$ also be a regular language. If so, then regular languages are closed under operation $x$ 
 \item Regular languages are closed under:
  \begin{itemise}
   \item Concatenation, Union
   \item Complementation (by switching all final and non-final states of an automaton)
   \item Intersection: $L_1 \cap L_2 =  \neg(\neg L_1 \cup \neg L_2)$
   \item NOTE: This allows us to modularize our FSA's for different purposes (e.g. we could have a FSA for recognizing prefixes and concatenate it with one for for recognizing root words [i.e. a {\bf Lexicon Automaton} and thus recognize a word like 'un + happy')
  \end{itemise}
\end{itemise}
- Some NLP applications of FSA's
\begin{itemise}
 \item Spell-checking
 \item POS tagging
 \item Stemming;
\end{itemise}
- Finite State Transducers
\begin{itemise}
 \item Goal: represent a word as a correspondence between two morphological levels:
  \begin{itemise}
   \item Lexical level: Abstract morphemes and features (e.g. cat +N +PL)
   \item Surface level: Actual spelling of the word (e.g. cats)
  \end{itemise}
 \item Accomplished by rewritting the input onto a second tape
 \item e.g.: 
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
× & c & a & t & +N & +PL & ×\\\hline
× & c & a & t & s & × & ×\\\hline
\end{tabular}
\end{center}
 \item Arcs are labeled with symbol pairs like $a:b, b:a, b:\epsilon, \epsilon:a,$ etc.
 \item Accepts a {\bf pair} of strings (e.g. $aaab:aabb$, or in the above example, $cat+N+PL:cats$)
 \item Defines a regular relation (e.g. $\{ a:b, aa:bb, aaa:bbb, ...\}$)
\end{itemise}
NOTE: Theres a fairly detailed group of slides at the end of the second slide set with example graphs  of using a transducer for doing derivational morphology like e-insertion


\newpage\section{Parsing (Zhang) 05.12-09.12}

% By Lamar
- Bottom-up vs. Top-Down parsing
\begin{itemise}
 \item Bottom-up - Start from input sequence and apply grammar rules to build tree upwards
 \item Top-down - Start from the start symbol and expand the tree with grammar rules
\end{itemise}
\subsection{ CKY Algorithm }
\begin{itemise}
 \item A bottom-up chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$ 
 \item Grammar must be in CNF
 \item Online javascript example: http://www.diotavelli.net/people/void/demos/cky.html
\end{itemise}
\subsection{ Earley Algorithm }
\begin{itemise}
 \item Top-Down chart parsing algorithm using dynamic programming
 \item Complexity $O(n^3)$
 \item Example (pdf, slides 18-36): \\http://www.coli.uni-saarland.de/~yzhang/rapt-ws1112/slides/schmidt.pdf
\end{itemise}
\subsection { Inside Outside Algorithm }
TODO


\newpage\section{Statistical NLP (Language Models) (Klakow, Wiegand) 12.12-23.12}

\subsection{Statistical Language Models}
\begin{itemise}
\item Speech Recognition: Speech signal $\rightarrow$ Feature extraction $\rightarrow$ Search: $W = argmax_w[P(a|w)P(w)]$ $\rightarrow$ Recognised word sequence W
\item $P(a|w)$ = acoustic model
\item $P(w)$ = language model
\item Information retrieval: Ponte, Croft, 1998: Query modelled by $P(Q|Data_i)$ (Simply - query given the data, the best fit wins.)
\item Perplexity and Error rate are correlated. 
\item Definition of Perplexity given, no information: \begin{equation} PP = P(w_1...w_N)^{-1/N}\end{equation}
\begin{equation} = exp(-\frac{1}{N}\sum_{w,h}N(w,h)log(P(w|h)))\end{equation}
Where $P(w|h)$ = language model, $N(w,h)$ = frequency of sequence in corpus, $N$  = size of corpus.
\item Minimize the perplexity on the training data (same as above, with $N_{train}$
\item MLE: \begin{equation}P(w|h) = \frac{N_{train}(w,h)}{N_{train}(h)}\end{equation}
\item Backing off language model: \begin{equation}P(w|h) = [\frac{N(w,h)-d}{N(h)} -\alpha(h)P(w|h\prime) for N(w,h) > 0\end{equation}
\begin{equation}P(w|h) = \alpha(h)P(w|h\prime)\end{equation}
Where $\alpha(h)$ is the backing off weight, $P(w|h\prime)$ is the backing off distribution, where $h\prime$ is the shortened history, and with a discounting parameter $d$.
\item Another improvement would be linear smoothing: \\
\begin{equation}P(w_0|w_{-1}) = \lambda_1\frac{N_{train}(w_{-1}w_0)}{N_{train}(w_{-1})} + \lambda_2\frac{N_{train}(w_0)}{N_{train}} + (1 - \lambda_1 - \lambda_2)\frac{1}{V (size of vocabulary)}\end{equation}
\item Marginal backing off (Knesser Ney Smoothing) reduces perplexity 10-20\%. 
\item Class language models: group words automatically, map them to classes, reduce parameters needed.
\item To build a state of the art system: Trigram, Absolute Discounting, Marginal Backing off, Linear interpolation with class model. 
\end{itemise}


\newpage\section{Prosodic Models for Speech Technology (Moebius) 09.01}

\newpage\section{Speech Synthesis (Moebius, Lasarcyk) 11.01-16.01}

\newpage\section{Automatic Speech Recognition (Moebius) 18.01}

\newpage\section{Corpora for speech technology (Lasarcyk) 20.01}

\newpage\section{Semantics (Pinkal) 23.01-3.02}
% By Lamar

\subsection{ Semantics I }
- Main tasks of semantic processing
\begin{itemise}
 \item How to represent word meanings, sentence meanings, compute sentence meanings from word meanings? $\rightarrow$ {\bf Semantic Construction}
 \item How to compute utterance meaning from ing. content and context information? $\rightarrow$ {\bf Disamibiguation or Ambiguity Resolution}
 \item How to obtain relevant utterance info from the intended utterance meaning? $\rightarrow$ {\bf Inference}
\end{itemise}

\subsection{The Predicate Logic}
- PL vocabulary
\begin{itemise}
 \item Logical symbols:
\begin{itemise}
 \item Connectives: $\neg, \vee, \wedge, \rightarrow, \leftrightarrow$
 \item Quantifiers: $\forall, \exists$
 \item Equality: $=$
\end{itemise}
 \item Infinite set of {\bf individual variable}: $VAR = \{x, y, z, ...\}$
 \item Arbitrary set of {\bf individual constants}: $CON = \{a, b, c,...\}$
 \item For every $n \geq 0$, an arbitrary, possibly empty set of n-ary {\bf predicate symbols}: $PRED^n = \{P, Q, ...\}$ 
 \item ${\bf TERM} = CON \cup VAR$
\end{itemise}
- Well-formed Formulae
\begin{enumerate}
 \item if $R$ is an n-ary predicate symbol, and $t_1 , ..., t_n$ are terms, then $R(t_1 , ..., t_n )$ is a wff
 \item if $t_1 , t_2$ are terms then $t_1 = t_2$ is a wff
 \item if $\phi, \psi$ are wff then $\neg\phi, (\phi\wedge\psi), (\phi\vee\psi), (\phi\rightarrow\psi), (\phi\leftrightarrow\psi)$ are wff
 \item if $\phi$ is a wff and $x$ an individual variable, then $\forall x\phi$ and $\exists x\phi$ are wff
\end{enumerate}
- Examples of atomic formulae
\begin{center}
\begin{tabular}{ll}
Bill works & work(bill)\\
Mary likes John & like(john, mary)\\
John introduces Bill to Mary & introduce(john, bill, mary)\\
Angela Merkel is the chancellor & angela\_merkel = the\_chancellor
\end{tabular}
\end{center}
- Models 
NOTE: This next part is is about as clear an explanation of models as I have ever seen, so I will just quote the whole slide directly 
\begin{itemise}
 \item FOL expressions are {\bf interpreted} wrt certain situations and states of the world
 \item These are schematically represented by relational structures which we call {\bf model structures} (more preciself: Model structures represent those properties of situations or states of the world, which are relevant to the terms and relation symbols occuring in an FOL fragment)
 \item Different types of FOL expressions (terms, relation symbols, formulae) are assigned appropriate constructs from the model structure, their {\bf denotations} by an {\bf interpretation function}
 \item In particular, formulae denote {\bf truth values}
 \item The {\bf truth conditions} of a formula are considered its meaning.
 \item A model structure is a pair $M = (U_M , V_M)$ where:
\begin{itemise}
 \item $U_M$ is the {\bf model universe}, a non empty set
 \item $V_M$ is a {\bf value assignment function} for basic expressions, which assigns...
\begin{itemise}
 \item $V_M (P) \subseteq U{_M}^n$, if $P$ is an n-ary predicate symbol
 \item $V_M (c) \in U_M$ if $c$ is a constant
\end{itemise} 
\end{itemise}
\end{itemise}
-Interpretation functions
\begin{itemise}
 \item An interpretation function $\llbracket\rrbracket ^{M,g}$ recursively assigns semantic values $\llbracket\alpha\rrbracket^{M,g}$ to all expressions $\alpha$ with respect to a model structure $M$ and a variable assignment $g$
 \item Interpretation of terms:
\begin{itemise}
 \item $\llbracket c\rrbracket^{M,g} = V_m (c)$ for al individual constants $c$
 \item $\llbracket X\rrbracket^{M,g} = g(X)$
\end{itemise}
 \item Interpretation of atomix expressions (`{\bf predicate-argument structures}')
\begin{itemise}
 \item $\llbracket R(t_1 , ..., t_n)\rrbracket^{M,g} = 1$ iff $(\llbracket t_1\rrbracket^{M,g}, ...,\llbracket t_n\rrbracket^{M,g}) \in V_M (R)$ %ugh...fuck you fol notation...
 \item $\llbracket t_1 = t_2\rrbracket^{M,g} = 1$ iff $\llbracket t_1\rrbracket^{M,g} = \llbracket t_2\rrbracket^{M,g}$
\end{itemise}
\end{itemise}
- An example
\begin{itemise}
 \item Input sentence: 'Bill works`
 \item Semantic construction returns formula: $work(bill)$
 \item Pred. logic interpretation gives the truth conditions: \\$\llbracket work(bill)\rrbracket^{M,g} = 1$ iff $\llbracket bill\rrbracket^{M,g} \in V_M (work)$ iff $V_M (bill) in V_M(work)$
 \item Then $work(bill)$ is true in a model structure $M$ iff the object denoted by $bull$ in $M$ is a member of the set denoted by `work' in $M$
\end{itemise}

<<<<<<< HEAD
\subsection{ Semantics II }
\subsection{ Semantics III }

\section{Discourse \& Dialogue (Sporleder) 06.02-08.02}
=======
\newpage\section{Discourse \& Dialogue (Sporleder) 06.02-08.02}
>>>>>>> ad477e0d4ae3f53f4a2c9613eb375c8a7dcfe9ed


\end{document}